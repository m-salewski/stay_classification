{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will reload imports before executing code, allowing you to easily change contents of custom scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stay classification: batch of synthetic training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('/home/sandm/Notebooks/stay_classification/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, 10)]\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the clusters which have a temporal gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR-plotting\n",
    "\n",
    "For each sub-cluster, plot the quantile boxes with whistkers.\n",
    "\n",
    "**Notes**\n",
    "* the boxes usually capture the baseline of the underlying stay\n",
    "* the forward and backward clusters\n",
    "    * usually the same clusters in the stays with similar IQRs\n",
    "    * usually different in the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here\n",
    "\n",
    "At this point, it seems that the basic clusters are formed. \n",
    "\n",
    "Next, use the IQRs of these clusters as the new bounds for extending the cluster: essentially using the extensible box method.\n",
    "\n",
    "Note that the IQR can be larger than the allow distance threshold; the box would therefore need to be the smaller of the two but with the same mean and/or median\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. `get_clusters_x` finds the clusters based on nearness of events in space and time\n",
    "2. `merge_clusters` merges neraby clusters based on ...\n",
    "3. `merge_clusters_2`  merges neraby clusters based on ...\n",
    "4. `extend_clusters` extend the clusters\n",
    "5. `separate_clusters` break the overlapping clusters and then re-merge\n",
    "6. `merge_clusters_2` merge the separated clusters\n",
    "7. `intersect` the forward and backward clusters\n",
    "\n",
    "### Notes\n",
    "\n",
    "Mostly, the results are good: more than 80% of the trajectories have prec and/or recall above 0.8\n",
    "\n",
    "The main issue\n",
    "* loss of precision due to non-stays being identified\n",
    "    * this (loss) is increased with higher event density but compensated by an increase in recall\n",
    "        * since many clusters are identified and the stay events have a higher probability of being classified as stay events.\n",
    "<br/>\n",
    "\n",
    "**TODO** since stay events are classified and not the accuracy of the stays, it would be useful to have a measure of the stay accuracy: _ie_ once a stay is identified, how much of that stay is correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper__3stays_v3_scripts import switch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data.plotting import plot_trajectory, add_plot_trajectory, add_plot_seg_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper__3stays_v3_scripts__plotting import plot_cluster_boxplots, add_plot_cluster_boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper__3stays_v3_scripts__eval import get_segments_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_clusts = lambda cluster_list : [print(f\"[{c[0]:4d},{c[-1]:4d}]\") for c in cluster_list]\n",
    "#print_ctimes = lambda cluster_list : [print(f\"[{time_arr[c[0]]:6.3f},{time_arr[c[-1]]:6.3f}]\") for c in cluster_list]\n",
    "#print_ctdiff = lambda cluster_list : [print(f\"{time_arr[c[-1]] - time_arr[c[0]]:6.3f}\") for c in cluster_list]\n",
    "#print_times = lambda l: list(map(lambda x: f\"{x:6.3f}\",l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data.trajectory import get_stay_segs, get_adjusted_stays\n",
    "#from synthetic_data.trajectory_class import get_rand_traj\n",
    "from synthetic_data.plotting import plot_trajectory, add_plot_seg_boxes\n",
    "\n",
    "from synthetic_data.trajectory_class import get_trajectory\n",
    "#from synthetic_data.trajectory import get_stay\n",
    "\n",
    "dsec = 1/3600.0\n",
    "t_total = np.arange(0,24,dsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data.canonical_stays import get3e, get3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper__get_clusters import get_clusters_1, get_clusters_2, get_clusters_3, get_clusters_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_thresh = 1/6\n",
    "dist_thresh=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event frac. =  0.002\n",
      "Dupli. frac. =  0.300\n"
     ]
    }
   ],
   "source": [
    "rand_range = lambda min_, max_, size: (max_-min_)*np.random.random_sample(size=size) + min_\n",
    "\n",
    "event_frac = rand_range(0.01,0.001, 1)[0]\n",
    "duplicate_frac = 0.30 #rand_range(1,0.3,0.05)[0]\n",
    "\n",
    "print(f\"Event frac. = {event_frac:6.3f}\\nDupli. frac. = {duplicate_frac:6.3f}\")\n",
    "\n",
    "configs = {\n",
    "    'time_thresh':1/6,\n",
    "    'dist_thresh':0.5,\n",
    "    'event_frac':event_frac,\n",
    "    'duplicate_frac':duplicate_frac,    \n",
    "    'noise_min':0.02,\n",
    "    'noise_max':0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_stay():\n",
    "    \n",
    "    rand_range = lambda min_, max_, size: (max_-min_)*np.random.random_sample(size=size) + min_\n",
    "\n",
    "    event_frac = rand_range(0.05,0.001, 1)[0]\n",
    "    duplicate_frac = 0.30 #rand_range(1,0.3,0.05)[0]\n",
    "\n",
    "    configs = {\n",
    "        'time_thresh':1/6,\n",
    "        'dist_thresh':0.5,\n",
    "        'event_frac':event_frac,\n",
    "        'duplicate_frac':duplicate_frac,    \n",
    "        'noise_min':0.02,\n",
    "        'noise_max':0.15\n",
    "    }\n",
    "\n",
    "    x_dist = rand_range(0.52,5.0, 10)[0]    \n",
    "    x_dist = (-1)**np.random.randint(0,2,1)*x_dist\n",
    "    \n",
    "    mid_len = rand_range(0.2, 8, 10)[0]\n",
    "    shift = rand_range(-5, 5, 21)[0]\n",
    "    \n",
    "    return configs, x_dist, mid_len, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: './testdata_training_set/'\n"
     ]
    }
   ],
   "source": [
    "from synthetic_data.trajectory_class import pickle_trajectory\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "date_tag = datetime.today().strftime('%Y%m%d')\n",
    "data_dir = f\"./testdata_{date_tag}_4_no_iqr_and_wi_ranges_and_wi_shift/\"\n",
    "\n",
    "data_dir = f\"./testdata_training_set/\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(data_dir)\n",
    "except OSError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "  20 of  1000\n",
      "  30 of  1000\n",
      "  40 of  1000\n",
      "  50 of  1000\n",
      "  60 of  1000\n",
      "  70 of  1000\n",
      "  80 of  1000\n",
      "  90 of  1000\n",
      " 100 of  1000\n",
      " 110 of  1000\n",
      " 120 of  1000\n",
      " 130 of  1000\n",
      " 140 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 150 of  1000\n",
      " 160 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 170 of  1000\n",
      " 180 of  1000\n",
      " 190 of  1000\n",
      " 200 of  1000\n",
      " 210 of  1000\n",
      " 220 of  1000\n",
      " 230 of  1000\n",
      " 240 of  1000\n",
      " 250 of  1000\n",
      " 260 of  1000\n",
      " 270 of  1000\n",
      " 280 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 290 of  1000\n",
      "Failed at 0 0\n",
      " 300 of  1000\n",
      " 310 of  1000\n",
      " 320 of  1000\n",
      " 330 of  1000\n",
      " 340 of  1000\n",
      " 350 of  1000\n",
      " 360 of  1000\n",
      " 370 of  1000\n",
      " 380 of  1000\n",
      " 390 of  1000\n",
      " 400 of  1000\n",
      " 410 of  1000\n",
      " 420 of  1000\n",
      " 430 of  1000\n",
      " 440 of  1000\n",
      " 450 of  1000\n",
      " 460 of  1000\n",
      " 470 of  1000\n",
      " 480 of  1000\n",
      " 490 of  1000\n",
      "Failed at 0 0\n",
      " 500 of  1000\n",
      " 510 of  1000\n",
      " 520 of  1000\n",
      " 530 of  1000\n",
      " 540 of  1000\n",
      "Failed at 0 0\n",
      " 550 of  1000\n",
      " 560 of  1000\n",
      " 570 of  1000\n",
      " 580 of  1000\n",
      " 590 of  1000\n",
      "Failed at 0 0\n",
      " 600 of  1000\n",
      " 610 of  1000\n",
      " 620 of  1000\n",
      " 630 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 640 of  1000\n",
      " 650 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 660 of  1000\n",
      " 670 of  1000\n",
      "Failed at 0 0\n",
      " 680 of  1000\n",
      "Failed at 0 0\n",
      " 690 of  1000\n",
      " 700 of  1000\n",
      "Failed at 0 0\n",
      " 710 of  1000\n",
      "Failed at 0 0\n",
      " 720 of  1000\n",
      " 730 of  1000\n",
      "Failed at 0 0\n",
      " 740 of  1000\n",
      " 750 of  1000\n",
      "Failed at 0 0\n",
      " 760 of  1000\n",
      " 770 of  1000\n",
      " 780 of  1000\n",
      " 790 of  1000\n",
      " 800 of  1000\n",
      " 810 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 820 of  1000\n",
      "Failed at 0 0\n",
      " 830 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 840 of  1000\n",
      "Failed at 0 0\n",
      " 850 of  1000\n",
      " 860 of  1000\n",
      "Failed at 0 0\n",
      " 870 of  1000\n",
      " 880 of  1000\n",
      "Failed at 0 0\n",
      " 890 of  1000\n",
      " 900 of  1000\n",
      "Failed at 0 0\n",
      " 910 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 920 of  1000\n",
      " 930 of  1000\n",
      "Failed at 0 0\n",
      " 940 of  1000\n",
      " 950 of  1000\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      "Failed at 0 0\n",
      " 960 of  1000\n",
      "Failed at 0 0\n",
      " 970 of  1000\n",
      " 980 of  1000\n",
      " 990 of  1000\n",
      "1000 of  1000\n"
     ]
    }
   ],
   "source": [
    "total = 1000\n",
    "ii = 0\n",
    "while ii < total:\n",
    "            \n",
    "    configs, x_dist, mid_len, shift = get_rand_stay()          \n",
    "\n",
    "    #if verbose: print(f\"{x_dist:6.3f}, {mid_len:6.3f}, {shift:6.3f}\")\n",
    "\n",
    "    val = np.random.randint(0,2,1)\n",
    "    if val:\n",
    "        stays = get3(x_dist, mid_len, shift)\n",
    "    else:\n",
    "        stays = get3e(x_dist, mid_len, shift)            \n",
    "\n",
    "    continuation = True\n",
    "    m = 0\n",
    "    while continuation:\n",
    "        n = 0\n",
    "        try:\n",
    "            time_arr, raw_arr, noise_arr, segments = get_trajectory(stays, t_total, configs)\n",
    "            t_segs, x_segs = get_stay_segs(get_adjusted_stays(segments, time_arr))\n",
    "            stays_tag = int((x_segs.size)/3)\n",
    "            continuation = False\n",
    "        except:\n",
    "            print(\"Failed at\",m,n)\n",
    "            if n > 10: \n",
    "                continuation = False\n",
    "            else:\n",
    "                n+=1\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        trajectory_tag = f\"trajectory_{stays_tag}stays__{ii}\"\n",
    "        path_to_file =  data_dir + trajectory_tag\n",
    "        pickle_trajectory(time_arr, raw_arr, noise_arr, segments, path_to_file + \".pkl\")\n",
    "        ii+=1\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    if ii % 10 == 0:\n",
    "        print(f\"{ii:4d} of {total:5d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  500 of 1000\n",
      "  550 of 1000\n",
      "  600 of 1000\n",
      "  650 of 1000\n",
      "  700 of 1000\n",
      "  750 of 1000\n",
      "  800 of 1000\n",
      "  850 of 1000\n",
      "  900 of 1000\n",
      "  950 of 1000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for ii in range(500, 1000):\n",
    "\n",
    "    stays_tag = 3\n",
    "    trajectory_tag = f\"trajectory_{stays_tag}stays__{ii}\"    \n",
    "    path_to_file =  data_dir + trajectory_tag\n",
    "    \n",
    "    trajectory = pickle.load( open(path_to_file + \".pkl\", \"rb\") )    \n",
    "    \n",
    "    segments = trajectory['segments']\n",
    "    t_arr = trajectory['time_arr']\n",
    "    r_arr = trajectory['raw_locs_arr']\n",
    "    x_arr = trajectory['nse_locs_arr']\n",
    "    t_segs, x_segs = get_stay_segs(get_adjusted_stays(segments, t_arr))\n",
    "\n",
    "    ax = plot_trajectory(t_arr, r_arr, x_arr, t_segs, x_segs, dist_thresh);\n",
    "    add_plot_seg_boxes(t_segs, x_segs, dist_thresh, ax)\n",
    "    ax.set_xlim([5.75,18.25]);\n",
    "\n",
    "    ylim = [noise_arr.min()-2*dist_thresh, noise_arr.max()+2*dist_thresh]\n",
    "\n",
    "    plt.savefig(path_to_file + \".png\")\n",
    "    plt.close()\n",
    "    \n",
    "    if ii % 50 == 0:\n",
    "        print(f\"{ii:5d} of 1000\")\n",
    "#'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
